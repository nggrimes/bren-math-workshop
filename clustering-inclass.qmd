---
title: "Bren Calculus Workshop"
execute: 
  echo: true
  warning: false
  message: false
  
format: 
  html:
    code-fold: true
---

```{r}
library(tidyverse)
library(googlesheets4)
library(knitr)
library(kableExtra)
```

## Introduction

## Data import

First we need to get the dat in from the Google form responses. For that we use the `googlesheets4` package. Normally you have to connect it to your gmail account the first time you use it, but I've already done that behind the scences. All we need to do is provide the google sheet url link.

```{r}
data<-read_sheet("https://docs.google.com/spreadsheets/d/1EwUbwXUpAQvpXuF_zvK4aCXOP9ucYuam_r3xBgZ6cKw/edit?resourcekey#gid=967303810") %>% 
  janitor::clean_names() %>% 
  select(-timestamp) |> 
  drop_na()
```


## Constrained K-means clustering

```{r}

colnames(data)<-c("specialization","major","math_level","use","relationship","name")
```

```{r}

likert<-data %>% 
  mutate(specialization=case_when(
    specialization=="Business and Sustainability"~1,
    specialization=="Environmental Policy"~2,
    specialization=="Coastal Resources and Management"~3,
    specialization=="Energy and Climate"~5,
    specialization=="Water Resources Management"~4,
    specialization=="Pollution Prevention and Remediation"~6,
    specialization=="Conservation Planning"~7
  )) %>% 
  mutate(major=case_when(
    major=="Humanities (History, Literature, Art)"~1,
    major=="Business"~2,
    major=="Political Science"~3,
    major=="Environmental Science"~4,
    major=="STEM (Biology, Chemistry, Physics)"~5,
    major=="Economics"~6,
    major=="Computer Science"~7
  )) %>% 
  mutate(math_level=case_when(
    math_level=="Algebra"~1,
    math_level=="Trigonometry"~2,
    math_level=="Calculus 1"~3,
    math_level=="Calculus 2"~4,
    math_level=="Calculus 3 and beyond (Diff eq, real analysis, if you know what those are click this option)"~5
  )) %>% 
  mutate(use=case_when(
    use=="Less than a year"~1,
    use=="1-2 years ago"~2,
    use=="3-5 years ago"~3,
    use=="Not since the dark times (5+ years)"~4
  )) %>% 
  mutate(relationship=case_when(
    relationship=="My mortal enemy"~1,
    relationship=="Your crazy, high key racist uncle you have to invite to Thanksgiving"~2,
    relationship=="The friend of a friend who's name you can never remember"~3,
    relationship=="Respectful co-worker you can have a beer with"~4,
    relationship=="My best friend!"~5
  ))

df<-likert %>% 
  select(-name) %>% 
  as.matrix() %>% 
  unname()
```



Now we can add some settings on how big we want the groups to be or how many groups we want. For example if out a class of 80 we want group sizes of 4 we would need to tell the algorithm to make 20 groups. Since there might be an uneven number (like 83) we'll tweak the max allowed in a group. For balanced sorting all groups must have at least one representative from each group. So the minimum size will be a rounding of the number of responses.

```{r}
group_size=4
groups=floor(length(df[,1])/group_size)
```

Unfortunately, the next step requires us to step away from R because no one has developed an appropriate package for the algorithm we're trying to use. Luckily for us, Quarto with `reticulate` makes running python and R code in the same workflow relatively easier. First we transfer the needed data from the R side to python suitable frames. Next we need to bring in the packages python needs to use for the algorithm. This is just like loading `library(tidyverse)` in R. Python also lets us name the package for our own personal uses. Personally, I rarely deviate from the authors recommendation. Next we set up the algorithm settings. Then call the alogrithm with a specific call much like we would in a TidyModels set up in R. We store the output in `labels` which is a vector of `[0:4]` of each students classifcation^[Reminder Python uses base 0. So the first index starts at 0 instead of 1 like in R]. 

```{python}
x=r.df
group=int(r.groups)
size=int(r.group_size)


from k_means_constrained import KMeansConstrained

import numpy as np

clf=KMeansConstrained(
  n_clusters=size,
  size_min=group,
  size_max=group+2,
  random_state=0
)



labels=clf.fit_predict(x)
```

Now we bring the labels from python to R with a quick `reticulate::py` call. To add true randomness into the assignments, I make a function to mix up the names of each person in the clusters, and then apply to all using `purrr:map`. From there it's a matter of converting the list into a usable dataframe while maintaining all the names. List data manipulation is almost more of an art than a science. Do whatever you need to do to make it work for your problem. Here I want to keep a general structure because I don't know how many people will fill out the form beforehand.


```{r}
library(reticulate)
clus_labels<-py$labels

likert$labels=clus_labels+1 ## REPLACE PRAC WITH LIKERT BY DEMO  Keep the add 1 becuase of python base 0

#prac$name=stringi::stri_rand_strings(length(prac$labels),6)


likert %>% 
  group_by(labels) %>% 
  summarize(n=n())

## mix up function

mix<-function(data,group){
  df_filter<-data %>% 
      filter(labels==group)
    
  out=sample(df_filter$name,size=length(df_filter$labels))
  
  return(out)
}


group_list<-1:group_size |>
  map(\(x) mix(likert,x))
  

tdf<-as.data.frame(t(plyr::ldply(group_list,rbind)))

#Adjust colnames to show teammate
colnames(tdf)<-c(paste0("teammate",1:group_size))


na_vec<-unlist(tdf[(groups+1):(length(tdf[,1])),]) %>% 
  na.omit()


add_vec<-c(na_vec,rep("NA",times=(groups-length(na_vec))))

tdf=tdf[1:groups,]


tdf$teammate_add<-add_vec

tdf$team_num=seq(1,length(add_vec))



```

```{r}
kable(tdf) %>% 
  kable_styling()
```



```{r}

```

